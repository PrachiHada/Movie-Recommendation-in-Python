{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Collaborative Filtering ALS Recommender System using Spark MLlib adapted from\n",
    "the Spark Summit 2014 Recommender System training example.\n",
    "\n",
    "Developed By: Pranav Masariya\n",
    "updated By: Prachi Hada\n",
    "Supervisor: Dr. Magdalini Eirinaki\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.ml.recommendation import ALS as mlals\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import math\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calling spark session to register application\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Recom\") \\\n",
    "    .config(\"spark.recom.demo\", \"1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading and Parsing Dataset\n",
    "    Each line in the ratings dataset (ratings.csv) is formatted as:\n",
    "         userId,movieId,rating,timestamp\n",
    "    Each line in the movies (movies.csv) dataset is formatted as:\n",
    "        movieId,title,genres\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "# Load ratings\n",
    "ratings_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"D:\\\\SJSU_courses\\\\256_Large_Scale_Analytics\\\\40 marks assignment\\\\PartC\\\\movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|   149|      2|     5|\n",
      "|   149|      3|     3|\n",
      "|   149|      5|     3|\n",
      "|   149|      7|     2|\n",
      "|   149|      9|     4|\n",
      "|   149|     10|     4|\n",
      "|   149|     11|     4|\n",
      "|   149|     12|     4|\n",
      "|   149|     13|     4|\n",
      "|   149|     14|     3|\n",
      "|   149|     16|     5|\n",
      "|   149|     18|     4|\n",
      "|   149|     20|     5|\n",
      "|   149|     21|     4|\n",
      "|   969|      5|     5|\n",
      "|   969|     14|     3|\n",
      "|   969|     15|     1|\n",
      "|   969|     16|     1|\n",
      "|   969|     21|     2|\n",
      "|   589|      1|     4|\n",
      "|   589|      2|     5|\n",
      "|   589|      3|     5|\n",
      "|   589|      4|     4|\n",
      "|   589|      5|     3|\n",
      "|   589|      6|     3|\n",
      "|   589|      7|     2|\n",
      "|   589|      8|     4|\n",
      "|   589|      9|     5|\n",
      "|   589|     10|     5|\n",
      "|   589|     11|     3|\n",
      "+------+-------+------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For the simplicity of this tutorial\n",
    "    For each line in the ratings dataset, we create a tuple of (UserID, MovieID, Rating). \n",
    "    We drop the timestamp because we do not need it for this recommender.\n",
    "\"\"\"\n",
    "\n",
    "#ratings_df = ratings_df.drop('timestamp')\n",
    "ratings_df.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load movies\n",
    "movies_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"D:\\\\SJSU_courses\\\\256_Large_Scale_Analytics\\\\40 marks assignment\\\\PartC\\\\movies21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieId|               title|\n",
      "+-------+--------------------+\n",
      "|      1|Rogue One / Star ...|\n",
      "|      2|          Fight Club|\n",
      "|      3|   Lord of the Rings|\n",
      "|      4|              Trolls|\n",
      "|      5|       Despicable Me|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor each line in the movies dataset, we create a tuple of (MovieID, Title). \\n    We drop the genres because we do not use them for this recommender.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each line in the movies dataset, we create a tuple of (MovieID, Title). \n",
    "    We drop the genres because we do not use them for this recommender.\n",
    "\"\"\"\n",
    "#Since the dataset which I created did not have genres, I did not have to drop it\n",
    "#movies_df = movies_df.drop('genres')\n",
    "#movies_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to determine the best ALS parameters, we will use the small dataset. \n",
    "We need first to split it into train, validation, and test datasets.\n",
    "\"\"\"\n",
    "(trainingData,validationData,testData) = ratings_df.randomSplit([0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare test and validation set. They should not have ratings\n",
    "\n",
    "validation_for_predict = validationData.select('userId','movieId')\n",
    "test_for_predict = testData.select('userId','movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spark MLlib library for Machine Learning provides a Collaborative Filtering implementation by \n",
    "using Alternating Least Squares. The implementation in MLlib has the following parameters:\n",
    "\n",
    "    1. numBlocks is the number of blocks used to parallelize computation (set to -1 to auto-configure).\n",
    "    2. rank is the number of latent factors in the model.\n",
    "    3. iterations is the number of iterations to run.\n",
    "    4. lambda specifies the regularization parameter in ALS.\n",
    "    5. implicitPrefs specifies whether to use the explicit \n",
    "        feedback ALS variant or one adapted for implicit feedback data.\n",
    "    6. alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline \n",
    "        confidence in preference observations.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "seed = 5 #Random seed for initial matrix factorization model. A value of None will use system time as the seed.\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1 #run for different lambdas - e.g. 0.01\n",
    "#ranks = [4, 8, 12] #number of features\n",
    "ranks = [12, 15, 20] #number of features\n",
    "errors = [0, 0, 0]\n",
    "#models = [0, 0, 0] #To fnd models#\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 12 the RMSE is 1.0058873144591214\n",
      "For rank 15 the RMSE is 0.995047166913738\n",
      "For rank 20 the RMSE is 0.987580514978236\n",
      "The best model was trained with rank 20\n"
     ]
    }
   ],
   "source": [
    "# Let us traing our dataset and check the best rank with lowest RMSE\n",
    "# predictAll method of the ALS takes only RDD format and hence we need to convert our dataframe into RDD\n",
    "# df.rdd will automatically converts Dataframe into RDD\n",
    "\n",
    "for rank in ranks:\n",
    "    model = ALS.train(trainingData, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(validation_for_predict.rdd).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = validationData.rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()) # RMSE Error\n",
    "    #print(\"err %d\" %(err))\n",
    "    #print(\"errors %d\"%(errors)\n",
    "    errors[err] = error\n",
    "    #models[err] = model\n",
    "    err += 1\n",
    "    print ('For rank %s the RMSE is %s' % (rank, error))\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "    \n",
    "\n",
    "print ('The best model was trained with rank %s' % best_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ALS.train(trainingData, best_rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 0.9985192323226102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe best part is we do not have to worry about RDD any more with this library\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark will soon deprecate MLLIb package. \n",
    "They are focusing more on ML packages with standard machine learning implementation\n",
    "Let's see that package also\n",
    "\"\"\"\n",
    "als =  mlals(maxIter=iterations,rank=4,seed=seed,regParam=regularization_parameter, userCol=\"userId\", itemCol=\"movieId\",ratingCol=\"rating\")\n",
    "modelML = als.fit(trainingData)\n",
    "pred = modelML.transform(validationData)\n",
    "pred = pred.where(pred['prediction'] != 'NaN')\n",
    "    \n",
    "# Evaluate the model by computing RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(pred)\n",
    "\n",
    "print ('RMSE is %s' % rmse)\n",
    "\n",
    "\"\"\"\n",
    "The best part is we do not have to worry about RDD any more with this library\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's take test dataset and get ratings\n",
    "predictions_test = model.predictAll(test_for_predict.rdd).map(lambda r: ((r[0], r[1]), r[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((200, 16), 2.4236910016183333),\n",
       " ((200, 20), 4.356737311120778),\n",
       " ((200, 21), 3.758480495816861)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## visualize preditions, here third element is predictions generated by ALS Model\n",
    "predictions_test.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's start recommending movies.\n",
    "I have written a method to call recommendations for a perticular user from test data\n",
    "\n",
    "TODO: You need to execute one more step before calling getRecommendations, \n",
    "      Think about that step. If you go through the seps below, you will realize it soon. - \n",
    "\"\"\"\n",
    "def getRecommendations(user,testDf,trainDf,model):\n",
    "    # get all user and his/her rated movies\n",
    "    userDf = testDf.filter(testDf.userId == user)\n",
    "    # filter movies from main set which have not been rated by selected user\n",
    "    # and pass it to model we sreated above\n",
    "    mov = trainDf.select('movieId').subtract(userDf.select('movieId'))\n",
    "    \n",
    "    # Again we need to covert our dataframe into RDD\n",
    "    pred_rat = model.predictAll(mov.rdd.map(lambda x: (user, x[0]))).collect()\n",
    "    \n",
    "    # Get the top 5 recommendations\n",
    "    recommendations = sorted(pred_rat, key=lambda x: x[2], reverse=True)[:5]\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies recommended for:858\n"
     ]
    }
   ],
   "source": [
    "# Assign user id for which we need recommendations\n",
    "user = 858\n",
    "\n",
    "# Call getRecommendations method\n",
    "derived_rec = getRecommendations(user,ratings_df,trainingData,model)\n",
    "\n",
    "print (\"Movies recommended for:%d\" % user)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=858, product=2, rating=4.840835691412698),\n",
       " Rating(user=858, product=12, rating=4.666836407620735),\n",
       " Rating(user=858, product=9, rating=4.653912486440795),\n",
       " Rating(user=858, product=10, rating=4.5528879725719555),\n",
       " Rating(user=858, product=19, rating=4.427450639981219)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derived_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+----------+\n",
      "|     title|\n",
      "+----------+\n",
      "|Fight Club|\n",
      "+----------+\n",
      "\n",
      "2\n",
      "+------------+\n",
      "|       title|\n",
      "+------------+\n",
      "|Pulp Fiction|\n",
      "+------------+\n",
      "\n",
      "3\n",
      "+----------------+\n",
      "|           title|\n",
      "+----------------+\n",
      "|The big Lebowski|\n",
      "+----------------+\n",
      "\n",
      "4\n",
      "+-------------+\n",
      "|        title|\n",
      "+-------------+\n",
      "|Almost Famous|\n",
      "+-------------+\n",
      "\n",
      "5\n",
      "+------------+\n",
      "|       title|\n",
      "+------------+\n",
      "|Ghostbusters|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "# TODO: we can convert derived_rec into a dataframe to present it properly\n",
    "for i in range(len(derived_rec)):\n",
    "    print (i+1)\n",
    "    movies_df.filter(movies_df.movieId==derived_rec[i][1]).select('title').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rec = spark.sparkContext.parallelize(derived_rec).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------------------+\n",
      "|user|product|            rating|\n",
      "+----+-------+------------------+\n",
      "| 858|      2| 4.840835691412698|\n",
      "| 858|     12| 4.666836407620735|\n",
      "| 858|      9| 4.653912486440795|\n",
      "| 858|     10|4.5528879725719555|\n",
      "| 858|     19| 4.427450639981219|\n",
      "+----+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_movies = df_rec.join(movies_df,df_rec.product == movies_df.movieId,\"inner\").select('user','movieId','title','rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------------+------------------+\n",
      "|user|movieId|           title|            rating|\n",
      "+----+-------+----------------+------------------+\n",
      "| 858|      2|      Fight Club| 4.840835691412698|\n",
      "| 858|     12|    Pulp Fiction| 4.666836407620735|\n",
      "| 858|      9|The big Lebowski| 4.653912486440795|\n",
      "| 858|     10|   Almost Famous|4.5528879725719555|\n",
      "| 858|     19|    Ghostbusters| 4.427450639981219|\n",
      "+----+-------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rec_movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
